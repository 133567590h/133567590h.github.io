

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="HRX">
  <meta name="keywords" content="">
  
    <meta name="description" content="hadoop入门学习（基础环境搭建）">
<meta property="og:type" content="article">
<meta property="og:title" content="hadoop(入门)">
<meta property="og:url" content="http://example.com/2024/03/12/hadoop-%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="小黄のblog">
<meta property="og:description" content="hadoop入门学习（基础环境搭建）">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/hadoop.jpg">
<meta property="article:published_time" content="2024-03-12T11:17:51.000Z">
<meta property="article:modified_time" content="2024-03-12T11:33:00.528Z">
<meta property="article:author" content="HRX">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/hadoop.jpg">
  
  
  
  <title>hadoop(入门) - 小黄のblog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/bg/hk416.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="hadoop(入门)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-03-12 19:17" pubdate>
          2024年3月12日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.9k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          66 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">hadoop(入门)</h1>
            
              <p id="updated-time" class="note note-info" style="">
                
                  
                    本文最后更新于 2024年3月12日 晚上
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h1><a name="hadoop-" class="anchor" href="#hadoop-"><span class="header-link"></span></a>Hadoop(入门)</h1><h2><a name="1-" class="anchor" href="#1-"><span class="header-link"></span></a>1 大数据概述</h2><h3><a name="1-1-" class="anchor" href="#1-1-"><span class="header-link"></span></a>1.1 概述</h3><p>大数据（Big Data）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和 处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化 能力的海量、高增长率和多样化的信息资产。</p>
<p><strong><em>大数据主要解决，海量数据的采集、存储和分析计算问题。</em></strong></p>
<p> 按顺序给出数据存储单位：bit、Byte、 KB、MB、GB、TB、PB、EB、ZB、YB、 BB、NB、DB。 </p>
<p>1Byte = 8bit 1K = 1024Byte 1MB = 1024K 1G = 1024M 1T = 1024G 1P = 1024T </p>
<h3><a name="1-2-" class="anchor" href="#1-2-"><span class="header-link"></span></a>1.2 特点</h3><ol>
<li><p>（<strong>Volume</strong>）大量</p>
<p>截至目前，人类生产的所有印刷材料的数据量是200PB，而历史上全人类总共 说过的话的数据量大约是5EB。当前，典型个人计算机硬盘的容量为TB量级，而 一些大企业的数据量已经接近EB量级。</p>
</li>
<li><p>（<strong>Velocity</strong>）高速</p>
<p>这是大数据区分于传统数据挖掘的最显著特征。根据IDC的“数字宇宙”的报 告，预计到2025年，全球数据使用量将达到163ZB。在如此海量的数据面前，处 理数据的效率就是企业的生命</p>
</li>
<li><p>（<strong>Variety</strong>）多样</p>
<p>这种类型的多样性也让数据被分为结构化数据和非结构化数据。相对于以往便于存储的 以数据库/文本为主的结构化数据，非结构化数据越来越多，包括网络日志、音频、视频、图 片、地理位置信息等，这些多类型的数据对数据的处理能力提出了更高要求</p>
</li>
<li><p>（<strong>Value</strong>）低价值密度</p>
<p>价值密度的高低与数据总量的大小成反比。</p>
<p><strong><em>它指的是在大量数据中真正有价值的信息往往只占很小的一部分，而大部分数据可能是噪声或者不相关的。因此，处理和分析大数据的一个关键挑战是如何从海量数据中提取出有用的信息。</em></strong></p>
</li>
<li><p>（<strong>Veracity</strong>）真实性</p>
<p>指大数据的质量，大数据的内容是与真实世界息息相关的，真实不一定代表准确，但一定不是虚假数据，这也是数据分析的基础。基于真实的交易与行为产生的数据，才有意义，如何Mock数据，是一个话题。如何识别造假数据，更是值得研究的领域。</p>
</li>
</ol>
<h3><a name="1-3-" class="anchor" href="#1-3-"><span class="header-link"></span></a>1.3 应用场景</h3><ul>
<li>抖音推荐</li>
<li>电商广告推荐</li>
<li>零售：分析用户消费习惯，为用户购买商品提供方便，从而提升商品销量。</li>
<li>物流仓储</li>
<li>保险</li>
<li>金融</li>
<li>房产</li>
<li>人工智能 + 5G + 物联网 + 虚拟与现实</li>
</ul>
<h3><a name="1-4-" class="anchor" href="#1-4-"><span class="header-link"></span></a>1.4 业务流程分析及内部组织结构</h3><h2><a name="2-hadoop-" class="anchor" href="#2-hadoop-"><span class="header-link"></span></a>2 Hadoop(入门)</h2><h3><a name="2-1-hadoop-" class="anchor" href="#2-1-hadoop-"><span class="header-link"></span></a>2.1 Hadoop概述</h3><h4><a name="2-1-1-hadoop" class="anchor" href="#2-1-1-hadoop"><span class="header-link"></span></a>2.1.1什么是hadoop</h4><p>1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构。 </p>
<p>2）主要解决，海量数据的存储和海量数据的分析计算问题。</p>
<p> 3）广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。</p>
<h4><a name="2-1-2-" class="anchor" href="#2-1-2-"><span class="header-link"></span></a>2.1.2发展历程</h4><p>查资料就行，了解，也是根据goole来的灵感</p>
<h4><a name="2-1-3-" class="anchor" href="#2-1-3-"><span class="header-link"></span></a>2.1.3三大发行版本</h4><p>Hadoop 三大发行版本：Apache、Cloudera、Hortonworks。 Apache 版本最原始（最基础）的版本，对于入门学习最好。2006 Cloudera 内部集成了很多大数据框架，对应产品 CDH。2008 Hortonworks 文档较好，对应产品 HDP。2011 Hortonworks 现在已经被 Cloudera 公司收购，推出新的品牌 CDP。</p>
<h4><a name="2-1-4hadoop-" class="anchor" href="#2-1-4hadoop-"><span class="header-link"></span></a>2.1.4hadoop优势</h4><p>1.高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元 素或存储出现故障，也不会导致数据的丢失。</p>
<p>2.高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。</p>
<p>3.高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处 理速度。</p>
<p>4.高容错性：能够自动将失败的任务重新分配</p>
<h4><a name="2-1-5hadoop-" class="anchor" href="#2-1-5hadoop-"><span class="header-link"></span></a>2.1.5hadoop组成</h4><p>在 Hadoop1.x 时 代 ， Hadoop中 的MapReduce同 时处理业务逻辑运算和资 源的调度，耦合性较大。 在Hadoop2.x时 代，增 加 了Yarn。Yarn只负责 资 源 的 调 度 ， MapReduce 只负责运算。 Hadoop3.x在组成上没 有变化。</p>
<h5><a name="2-1-5-1-hdfs" class="anchor" href="#2-1-5-1-hdfs"><span class="header-link"></span></a>2.1.5.1 HDFS</h5><p>Hadoop Distributed File System，简称 HDFS，是一个分布式文件系统。</p>
<p>HDFS架构概述</p>
<p>NameNode（nn）：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、 文件权限），以及每个文件的块列表和块所在的DataNode等。</p>
<p>DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。</p>
<p>Secondary NameNode(2nn)：每隔一段时间对NameNode元数据备份。</p>
<h5><a name="2-1-5-2-yarn" class="anchor" href="#2-1-5-2-yarn"><span class="header-link"></span></a>2.1.5.2 YARN</h5><p>Yet Another Resource Negotiator 简称 YARN ，另一种资源协调者，是 Hadoop 的资源管理器</p>
<p>YARN架构概述</p>
<p>ResourceManager（RM）：整个集群资源（内存、CPU等）的老大 </p>
<p>ApplicationMaster（AM）：单个任务运行的老大 </p>
<p>NodeManager（N M）：单个节点服务器资源老大 </p>
<p>Container：容器，相当一台独立的服务器，里面封装了 任务运行所需要的资源，如内存、CPU、磁盘、网络等。</p>
<p><strong><em>说明1：客户端可以有多个 说明2：集群上可以运行多个ApplicationMaster 说明3：每个NodeManager上可以有多个Container</em></strong></p>
<h5><a name="2-1-5-3-mapreduce" class="anchor" href="#2-1-5-3-mapreduce"><span class="header-link"></span></a>2.1.5.3 MapReduce</h5><p>MapReduce 将计算过程分为两个阶段：Map 和 Reduce</p>
<p>Map 阶段并行处理输入数据</p>
<p>Reduce 阶段对 Map 结果进行汇总</p>
<h5><a name="2-1-5-4-" class="anchor" href="#2-1-5-4-"><span class="header-link"></span></a>2.1.5.4 三者关系</h5><p><img src="/2024/03/12/hadoop-%E5%85%A5%E9%97%A8/image-20240311195245261-17102425186731.png" srcset="/img/loading.gif" lazyload alt="image-20240311195245261"></p>
<h4><a name="2-1-6-" class="anchor" href="#2-1-6-"><span class="header-link"></span></a>2.1.6大数据技术生态体系</h4><h5><a name="-" class="anchor" href="#-"><span class="header-link"></span></a>推荐系统框架图</h5><p><img src="/2024/03/12/hadoop-%E5%85%A5%E9%97%A8/image-20240311195318640-17102425340403.png" srcset="/img/loading.gif" lazyload alt="image-20240311195318640"></p>
<p>图中涉及的技术名词解释如下：</p>
<p> 1）Sqoop：Sqoop 是一款开源的工具，主要用于在 Hadoop、Hive 与传统的数据库（MySQL） 间进行数据的传递，可以将一个关系型数据库（例如 ：MySQL，Oracle 等）中的数据导进 到 Hadoop 的 HDFS 中，也可以将 HDFS 的数据导进到关系型数据库中。</p>
<p> 2）Flume：Flume 是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统， Flume 支持在日志系统中定制各类数据发送方，用于收集数据；</p>
<p> 3）Kafka：Kafka 是一种高吞吐量的分布式发布订阅消息系统；</p>
<p>4）Spark：Spark 是当前最流行的开源大数据内存计算框架。可以基于 Hadoop 上存储的大数 据进行计算。</p>
<p> 5）Flink：Flink 是当前最流行的开源大数据内存计算框架。用于实时计算的场景较多。 </p>
<p>6）Oozie：Oozie 是一个管理 Hadoop 作业（job）的工作流程调度管理系统。</p>
<p> 7）Hbase：HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库， 它是一个适合于非结构化数据存储的数据库。 </p>
<p>8）Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张 数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运 行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开 发专门的 MapReduce 应用，十分适合数据仓库的统计分析。</p>
<p> 9）ZooKeeper：它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、 名字服务、分布式同步、组服务等。</p>
<h2><a name="3-hadoop-" class="anchor" href="#3-hadoop-"><span class="header-link"></span></a>3 Hadoop 运行环境搭建（开发重点）</h2><h3><a name="3-1-strong-centos-7-5-x86-1804-strong-" class="anchor" href="#3-1-strong-centos-7-5-x86-1804-strong-"><span class="header-link"></span></a>3.1 模板虚拟机环境准备<strong>CentOS-7.5-x86-1804为例</strong></h3><h4><a name="3-1-1-" class="anchor" href="#3-1-1-"><span class="header-link"></span></a>3.1.1 基础准备</h4><p>安装好vm，并下载CentOS的镜像文件，创建一个虚拟机，配置好机器配置，完成好后，载入CentOS，启动！</p>
<p>上述涉及Linux的知识。见Linux的博客</p>
<h5><a name="3-1-1-1-" class="anchor" href="#3-1-1-1-"><span class="header-link"></span></a>3.1.1.1模板虚拟机准备</h5><p>完成开机后，需要将虚拟机设置为静态ip</p>
<p>1.首先进入命令行，输入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim   /etc/sysconfig/network-scripts/ifcfg-ens33<br></code></pre></td></tr></table></figure>
<p>进入文件，按i进入编辑模式</p>
<p>修改并添加下列设置</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vim">BOOTPROTO=<span class="hljs-string">&quot;static&quot;</span> <br>ONBOOT=<span class="hljs-string">&quot;yes&quot;</span>   #系统启动的时候网络接口是否有效（yes/<span class="hljs-keyword">no</span>）<br>#IP地址<br>IPADDR=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.100</span>  <br>#网关  <br>GATEWAY=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.2</span>      <br>#域名解析器<br>DNS1=<span class="hljs-number">192.168</span>.<span class="hljs-number">10.2</span><br>修改成自己的也行，但要保证<span class="hljs-number">168</span>后面的字段一致<br></code></pre></td></tr></table></figure>
<p>修改完后，按Esc 并输入:wq，保存并退出</p>
<p>2.接着输入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl restart network<br></code></pre></td></tr></table></figure>
<p>重启网络服务。如果报错，则执行“reboot”命令，重启虚拟机</p>
<p>3.使用ifconfig命令查看当前IP</p>
<p>并查看ens33处ip是否为修改后的IPADDR</p>
<p>4.修改vm处的网络编辑器处vm8的网段相关信息与虚拟机一致。</p>
<h5><a name="3-1-1-2-hosts-" class="anchor" href="#3-1-1-2-hosts-"><span class="header-link"></span></a>3.1.1.2 修改主机名和hosts文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/hostname 修改为自己设置的主机名hadoop100<br></code></pre></td></tr></table></figure>
<p><strong>配置Linux</strong>克隆机主机名称映射hosts文件，打开/etc/hosts</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/hosts<br>并输入<br>192.168.10.100 hadoop100<br>192.168.10.101 hadoop101<br>192.168.10.102 hadoop102<br>192.168.10.103 hadoop103<br>192.168.10.104 hadoop104<br>192.168.10.105 hadoop105<br>192.168.10.106 hadoop106<br>192.168.10.107 hadoop107<br>192.168.10.108 hadoop108<br><br>完成后重启<br></code></pre></td></tr></table></figure>
<p><strong>修改windows</strong>的主机映射文件（host文件）</p>
<p>进入C:\Windows\System32\drivers\etc路径</p>
<p>打开hosts文件并添加如下内容，然后保存</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">192.168.10.100 hadoop100<br>192.168.10.101 hadoop101<br>192.168.10.102 hadoop102<br>192.168.10.103 hadoop103<br>192.168.10.104 hadoop104<br>192.168.10.105 hadoop105<br>192.168.10.106 hadoop106<br>192.168.10.107 hadoop107<br>192.168.10.108 hadoop108<br></code></pre></td></tr></table></figure>
<p>ps:如果是win10及以上则需另存后再覆盖</p>
<h4><a name="3-1-2-" class="anchor" href="#3-1-2-"><span class="header-link"></span></a>3.1.2 远程登录</h4><p>安装xshell并与模板机进行连接</p>
<p>完成上述准备工作</p>
<h3><a name="3-2-" class="anchor" href="#3-2-"><span class="header-link"></span></a>3.2虚拟机配置</h3><ol>
<li><p>安装epel-release</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install -y epel-release<br>注意：如果Linux安装的是最小系统版，还需要安装如下工具；如果安装的是Linux桌面标准版，不需要执行如下操作<br>yum install -y net-tools <br>yum install -y vim<br></code></pre></td></tr></table></figure>
</li>
<li><p>关闭防火墙，关闭防火墙开机自启</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl stop firewalld<br>systemctl disable firewalld.service<br>注意：在企业开发时，通常单个服务器的防火墙时关闭的。公司整体对外会设置非常安全的防火墙<br></code></pre></td></tr></table></figure>
</li>
<li><p>创建atguigu用户</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">useradd atguigu<br>passwd atguigu<br></code></pre></td></tr></table></figure>
<p>如果在创建虚拟机的时候已经创建用户，则跳过此步</p>
</li>
<li><p>配置atguigu</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/sudoers<br>修改/etc/sudoers文件，在%wheel这行下面添加一行，如下所示：<br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># Allow root to run any commands anywhere</span></span><br>root    ALL=(ALL)     ALL<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># Allows people in group wheel to run all commands</span></span><br><span class="hljs-meta prompt_">%</span><span class="language-bash">wheel  ALL=(ALL)       ALL</span><br>atguigu   ALL=(ALL)     NOPASSWD:ALL<br>注意：atguigu这一行不要直接放到root行下面，因为所有用户都属于wheel组，你先配置了atguigu具有免密功能，但是程序执行到%wheel行时，该功能又被覆盖回需要密码。所以atguigu要放到%wheel这行下面。<br></code></pre></td></tr></table></figure>
</li>
<li><p>在/opt目录下创建文件夹，并修改所属主和所属组</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">在/opt目录下创建module、software文件夹<br>mkdir /opt/module<br>mkdir /opt/software<br>修改module、software文件夹的所有者和所属组均为atguigu用户 <br>chown atguigu:atguigu /opt/module <br>chown atguigu:atguigu /opt/software<br>查看module、software文件夹的所有者和所属组<br>cd /opt/<br>ll<br>drwxr-xr-x. 2 atguigu atguigu 4096 5月  28 17:18 module<br>drwxr-xr-x. 2 root    root    4096 9月   7 2017 rh<br>drwxr-xr-x. 2 atguigu atguigu 4096 5月  28 17:18 software<br></code></pre></td></tr></table></figure>
</li>
<li><p>卸载虚拟机自带的JDK</p>
<p>如果你的虚拟机是最小化安装不需要执行这一步。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps <br>	rpm -qa：查询所安装的所有rpm软件包<br>	grep -i：忽略大小写<br>	xargs -n1：表示每次只传递一个参数<br>	rpm -e –nodeps：强制卸载软件<br></code></pre></td></tr></table></figure>
</li>
<li><p>重启虚拟机</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">reboot<br></code></pre></td></tr></table></figure>
</li>
</ol>
<h3><a name="3-3-" class="anchor" href="#3-3-"><span class="header-link"></span></a>3.3 克隆虚拟机</h3><h4><a name="3-3-1-" class="anchor" href="#3-3-1-"><span class="header-link"></span></a>3.3.1克隆</h4><p><strong>利用模板机hadoop100</strong>，克隆三台虚拟机：hadoop102 hadoop103 hadoop104</p>
<p>注意：克隆时，要先关闭hadoop100</p>
<h4><a name="3-3-2-jdk" class="anchor" href="#3-3-2-jdk"><span class="header-link"></span></a>3.3.2安装JDK</h4><p>注意：安装JDK前，一定确保提前删除了虚拟机自带的JDK。</p>
<p><strong>用XShell</strong>传输工具将JDK<strong>导入到opt</strong>目录下面的software<strong>文件夹下面</strong></p>
<p>在 Linux 系统下的 opt 目录中查看软件包是否导入成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ls /opt/software/<br></code></pre></td></tr></table></figure>
<p>解压 JDK 到/opt/module 目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf jdk-8u212-linux-x64.tar.gz -C /opt/module/<br></code></pre></td></tr></table></figure>
<p>配置 JDK 环境变量</p>
<p>新建/etc/profile.d/my_env.sh 文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim /etc/profile.d/my_env.sh<br>添加如下内容<br><span class="hljs-meta prompt_">#</span><span class="language-bash">JAVA_HOME</span><br>export JAVA_HOME=/opt/module/jdk1.8.0_212<br>export PATH=$PATH:$JAVA_HOME/bin<br></code></pre></td></tr></table></figure>
<p>保存后退出</p>
<p>source 一下/etc/profile 文件，让新的环境变量 PATH 生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure>
<p>测试 JDK 是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">java -version<br>如果能看到以下结果，则代表 Java 安装成功。<br>java version &quot;1.8.0_212&quot;<br></code></pre></td></tr></table></figure>
<p>注意：重启（如果 java -version 可以用就不用重启）</p>
<h4><a name="3-3-3-hadoop102-hadoop" class="anchor" href="#3-3-3-hadoop102-hadoop"><span class="header-link"></span></a>3.3.3在hadoop102安装hadoop</h4><p>用 XShell 文件传输工具将 hadoop-3.1.3.tar.gz 导入到 opt 目录下面的 software 文件夹下面</p>
<p>进入到 Hadoop 安装包路径下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /opt/software/<br></code></pre></td></tr></table></figure>
<p>解压安装文件到/opt/module 下面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf hadoop-3.1.3.tar.gz -C <br>/opt/module/<br></code></pre></td></tr></table></figure>
<p>查看是否解压成功</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"> <span class="hljs-built_in">ls</span> /opt/module/<br>显示<br>hadoop-3.1.3<br></code></pre></td></tr></table></figure>
<p>将 Hadoop 添加到环境变量</p>
<p>打开/etc/profile.d/my_env.sh 文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim<br>/etc/profile.d/my_env.sh<br></code></pre></td></tr></table></figure>
<p>在 my_env.sh 文件末尾添加如下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">HADOOP_HOME</span><br>export HADOOP_HOME=/opt/module/hadoop-3.1.3<br>export PATH=$PATH:$HADOOP_HOME/bin<br>export PATH=$PATH:$HADOOP_HOME/sbin<br></code></pre></td></tr></table></figure>
<p>让修改后的文件生效</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure>
<p>测试是否安装成功</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">hadoop version<br>出现hadoop的版号<br></code></pre></td></tr></table></figure>
<p>重启（如果 Hadoop 命令不能用再重启虚拟机）</p>
<h3><a name="3-4-hadoop-" class="anchor" href="#3-4-hadoop-"><span class="header-link"></span></a>3.4 hadoop 目录结构</h3><p>查看 Hadoop 目录结构</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">drwxr-xr-x. 2 atguigu atguigu 4096 5 月 22 2017 bin<br>drwxr-xr-x. 3 atguigu atguigu 4096 5 月 22 2017 etc<br>drwxr-xr-x. 2 atguigu atguigu 4096 5 月 22 2017 include<br>drwxr-xr-x. 3 atguigu atguigu 4096 5 月 22 2017 lib<br>drwxr-xr-x. 2 atguigu atguigu 4096 5 月 22 2017 libexec<br>-rw-r--r--. 1 atguigu atguigu 15429 5 月 22 2017 LICENSE.txt<br>-rw-r--r--. 1 atguigu atguigu 101 5 月 22 2017 NOTICE.txt<br>-rw-r--r--. 1 atguigu atguigu 1366 5 月 22 2017 README.txt<br>drwxr-xr-x. 2 atguigu atguigu 4096 5 月 22 2017 sbin<br>drwxr-xr-x. 4 atguigu atguigu 4096 5 月 22 2017 share<br></code></pre></td></tr></table></figure>
<p>（1）bin 目录：存放对 Hadoop 相关服务（hdfs，yarn，mapred）进行操作的脚本 </p>
<p>（2）etc 目录：Hadoop 的配置文件目录，存放 Hadoop 的配置文件 </p>
<p>（3）lib 目录：存放 Hadoop 的本地库（对数据进行压缩解压缩功能） </p>
<p>（4）sbin 目录：存放启动或停止 Hadoop 相关服务的脚本 </p>
<p>（5）share 目录：存放 Hadoop 的依赖 jar 包、文档、和官方案例</p>
<h2><a name="4-hadoop-" class="anchor" href="#4-hadoop-"><span class="header-link"></span></a>4 Hadoop 运行模式</h2><p>Hadoop 运行模式包括：本地模式、伪分布式模式以及完全分布式模式。</p>
<p>本地模式：单机运行，只是用来演示一下官方案例。生产环境不用。</p>
<p>伪分布式模式：也是单机运行，但是具备 Hadoop 集群的所有功能，一台服务器模 拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用。</p>
<p>完全分布式模式：多台服务器组成分布式环境。生产环境使用。</p>
<h3><a name="4-1-wordcount-" class="anchor" href="#4-1-wordcount-"><span class="header-link"></span></a>4.1本地运行模式（官方 WordCount）</h3><p>创建在 hadoop-3.1.3 文件下面创建一个 wcinput 文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir wcinput<br></code></pre></td></tr></table></figure>
<p>在 wcinput 文件下创建一个 word.txt 文件</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">cd</span> wcinput<br><span class="hljs-keyword">vim</span> word.txt<br>在文件中输入如下内容<br>hadoop yarn<br>hadoop mapreduce<br>atguigu<br>atguigu<br>保存退出<br></code></pre></td></tr></table></figure>
<p>回到 Hadoop 目录/opt/module/hadoop-3.1.3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">hadoop jar <br>share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar <br>wordcount wcinput wcoutput<br></code></pre></td></tr></table></figure>
<p>查看结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat wcoutput/part-r-00000<br>看到如下结果：<br>atguigu 2<br>hadoop 2<br>mapreduce 1<br>yarn 1<br></code></pre></td></tr></table></figure>
<h3><a name="4-2-" class="anchor" href="#4-2-"><span class="header-link"></span></a>4.2 完全分布式运行模式（开发重点）</h3><p>1）准备 3 台客户机（关闭防火墙、静态 IP、主机名称）</p>
<p> 2）安装 JDK</p>
<p> 3）配置环境变量</p>
<p> 4）安装 Hadoop </p>
<p>5）配置环境变量</p>
<p> 6）配置集群</p>
<p>7）单点启动 </p>
<p>8）配置 ssh </p>
<p>9）群起并测试集群</p>
<p> 虚拟机准备,前面的章节，现在应该是拥有4台虚拟机Hadoop100 (模板机) ，hadoop102,hadoop103,hadoop104</p>
<h3><a name="4-2-1-xsync" class="anchor" href="#4-2-1-xsync"><span class="header-link"></span></a>4.2.1编写集群分发脚本 xsync</h3><h4><a name="4-2-1-1-scp-secure-copy-" class="anchor" href="#4-2-1-1-scp-secure-copy-"><span class="header-link"></span></a>4.2.1.1 scp（secure copy）安全拷贝</h4><p>（1）scp 定义 scp 可以实现服务器与服务器之间的数据拷贝。（from server1 to server2） </p>
<p>（2）基本语法</p>
<figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mel">scp -r $pdir/$fname $user@$host:$pdir/$fname  <br>命令 递归 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称<br></code></pre></td></tr></table></figure>
<p>（3）案例实操</p>
<p><strong>前提：在 hadoop102、hadoop103、hadoop104 都已经创建好的/opt/module、  /opt/software 两个目录，并且已经把这两个目录修改为 atguigu:atguigu</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo chown atguigu:atguigu -R /opt/module<br>（a）在 hadoop102 上，将 hadoop102 中/opt/module/jdk1.8.0_212 目录拷贝到hadoop103 上。<br>scp -r /opt/module/jdk1.8.0_212     atguigu@hadoop103:/opt/module<br><br>（b）在 hadoop103 上，将 hadoop102 中/opt/module/hadoop-3.1.3 目录拷贝到hadoop103 上。<br>scp -r  atguigu@hadoop102:/opt/module/hadoop-3.1.3   /opt/module/<br><br>（c）在 hadoop103 上操作，将 hadoop102 中/opt/module 目录下所有目录拷贝到hadoop104 上。<br>scp -r <br>atguigu@hadoop102:/opt/module/*<br>atguigu@hadoop104:/opt/module<br><br></code></pre></td></tr></table></figure>
<h4><a name="4-2-1-2-rsync-" class="anchor" href="#4-2-1-2-rsync-"><span class="header-link"></span></a>4.2.1.2 rsync 远程同步工具</h4><p>rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点。 rsync 和 scp 区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更 新。scp 是把所有文件都复制过去。</p>
<p>（1）基本语法 </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">rsync -av $pdir/$fname $user@$host:$pdir/$fname <br>命令 选项参数 要拷贝的文件路径/名称 目的地用户@主机:目的地路径/名称 <br>    选项参数说明:       选项 功能<br>                     -a 归档拷贝<br>                     -v 显示复制过程<br></code></pre></td></tr></table></figure>
<p>（2）案例实操</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">（a）删除 hadoop103 中/opt/module/hadoop-3.1.3/wcinput<br> rm -rf wcinput/<br><br>（b）同步 hadoop102 中的/opt/module/hadoop-3.1.3 到 hadoop103<br>rsync -av hadoop-3.1.3/ <br>atguigu@hadoop103:/opt/module/hadoop-3.1.3/<br></code></pre></td></tr></table></figure>
<h4><a name="4-2-1-3-xsync-" class="anchor" href="#4-2-1-3-xsync-"><span class="header-link"></span></a>4.2.1.3 xsync 集群分发脚本</h4><p>（1）需求：循环复制文件到所有节点的相同目录下 </p>
<p>（2）需求分析： （a）rsync 命令原始拷贝： </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">rsync -av /opt/module atguigu@hadoop103:/opt/ <br></code></pre></td></tr></table></figure>
<p>（b）期望脚本： xsync 要同步的文件名称 </p>
<p>（c）期望脚本在任何路径都能使用（脚本放在声明了全局环境变量的路径） </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo $PATH /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/atgu igu/.local/bin:/home/atguigu/bin:/opt/module/jdk1.8.0_212/bi n <br></code></pre></td></tr></table></figure>
<p>（3）脚本实现 </p>
<p>（a）在/home/atguigu/bin 目录下创建 xsync 文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs shell"> cd /home/atguigu <br> mkdir bin <br> cd bin <br> vim xsync<br> <br> 在该文件中编写如下代码 :<br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">1. 判断参数个数</span><br>if [ $# -lt 1 ]<br>then<br>    echo Not Enough Arguement!<br>    exit;<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">2. 遍历集群所有机器</span><br>for host in hadoop102 hadoop103 hadoop104<br>do<br>    echo ====================  $host  ====================<br>    #3. 遍历所有目录，挨个发送<br><br>    for file in $@<br>    do<br>        #4. 判断文件是否存在<br>        if [ -e $file ]<br>            then<br>                #5. 获取父目录<br>                pdir=$(cd -P $(dirname $file); pwd)<br><br>                #6. 获取当前文件的名称<br>                fname=$(basename $file)<br>                ssh $host &quot;mkdir -p $pdir&quot;<br>                rsync -av $pdir/$fname $host:$pdir<br>            else<br>                echo $file does not exists!<br>        fi<br>    done<br>done<br></code></pre></td></tr></table></figure>
<p>（b）修改脚本 xsync 具有执行权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">chmod +x xsync或 chmod 777 xsync<br></code></pre></td></tr></table></figure>
<p>（c）测试脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">xsync /home/atguigu/bin<br></code></pre></td></tr></table></figure>
<p>（d）将脚本复制到/bin中，以便全局调用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo cp xsync /bin/<br></code></pre></td></tr></table></figure>
<p>（e）同步环境变量配置（root所有者）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo ./bin/xsync /etc/profile.d/my_env.sh<br>注意：如果用了sudo，那么xsync一定要给它的路径补全。<br>让环境变量生效:在对应的另外两台机器上进行<br>source /etc/profile<br>source /etc/profile<br></code></pre></td></tr></table></figure>
<h3><a name="4-2-2-ssh-" class="anchor" href="#4-2-2-ssh-"><span class="header-link"></span></a>4.2.2 SSH无密登录配置</h3><p><strong>1）配置ssh</strong></p>
<p>（1）基本语法</p>
<p>ssh另一台电脑的IP地址</p>
<p>（2）ssh连接时出现Host key verification failed的解决方法</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[atguigu@hadoop102 ~]$ ssh hadoop103<br>	如果出现如下内容<br>Are you sure you want to continue connecting (yes/no)? <br>	输入yes，并回车<br>	进入到hadoop103<br></code></pre></td></tr></table></figure>
<p>（3）退回到hadoop102</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[atguigu@hadoop103 ~]$ exit<br></code></pre></td></tr></table></figure>
<p><strong>2）无密钥配置</strong></p>
<p>（1）免密登录原理</p>
<p>（2）生成公钥和私钥</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"> pwd<br>/home/atguigu/.ssh<br><br> ssh-keygen -t rsa<br><br>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）<br></code></pre></td></tr></table></figure>
<p>（3）将公钥拷贝到要免密登录的目标机器上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102<br>[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103<br>[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104<br>注意：<br>还需要在hadoop103上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。<br>还需要在hadoop104上采用atguigu账号配置一下无密登录到hadoop102、hadoop103、hadoop104服务器上。<br>还需要在hadoop102上采用root账号，配置一下无密登录到hadoop102、hadoop103、hadoop104；<br>与上述命令相似<br></code></pre></td></tr></table></figure>
<p><strong>3）.ssh文件夹下（~/.ssh）的文件功能解释</strong></p>
<table>
<thead>
<tr>
<th>known_hosts</th>
<th>记录ssh访问过计算机的公钥（public  key）</th>
</tr>
</thead>
<tbody>
<tr>
<td>id_rsa</td>
<td>生成的私钥</td>
</tr>
<tr>
<td>id_rsa.pub</td>
<td>生成的公钥</td>
</tr>
<tr>
<td>authorized_keys</td>
<td>存放授权过的无密登录服务器公钥</td>
</tr>
</tbody>
</table>
<h3><a name="4-2-3-" class="anchor" href="#4-2-3-"><span class="header-link"></span></a>4.2.3 集群配置</h3><p><strong>1）集群部署规划</strong></p>
<p> <strong><em>NameNode和SecondaryNameNode不要安装在同一台服务器</em></strong></p>
<p><strong><em>ResourceManager也很消耗内存，不要和NameNode、SecondaryNameNode配置在同一台机器上。</em></strong></p>
<table>
<thead>
<tr>
<th></th>
<th>hadoop102</th>
<th>hadoop103</th>
<th>hadoop104</th>
</tr>
</thead>
<tbody>
<tr>
<td>HDFS</td>
<td>NameNode  DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode  DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>ResourceManager  NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody>
</table>
<p><strong>2）配置文件说明</strong></p>
<p>Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。</p>
<p>（1）默认配置文件：</p>
<table>
<thead>
<tr>
<th>要获取的默认文件</th>
<th>文件存放在Hadoop的jar包中的位置</th>
</tr>
</thead>
<tbody>
<tr>
<td>[core-default.xml]</td>
<td>hadoop-common-3.1.3.jar/core-default.xml</td>
</tr>
<tr>
<td>[hdfs-default.xml]</td>
<td>hadoop-hdfs-3.1.3.jar/hdfs-default.xml</td>
</tr>
<tr>
<td>[yarn-default.xml]</td>
<td>hadoop-yarn-common-3.1.3.jar/yarn-default.xml</td>
</tr>
<tr>
<td>[mapred-default.xml]</td>
<td>hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml</td>
</tr>
</tbody>
</table>
<p>（2）自定义配置文件：</p>
<p>​    <strong>core-site.xml</strong>、<strong>hdfs-site.xml</strong>、<strong>yarn-site.xml</strong>、<strong>mapred-site.xml</strong>四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。</p>
<p><strong>3）配置集群</strong></p>
<p>（1）核心配置文件</p>
<p>配置core-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd $HADOOP_HOME/etc/hadoop<br>vim core-site.xml<br><br>文件内容如下：<br>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;<br>&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;<br><br>&lt;configuration&gt;<br>    &lt;!-- 指定NameNode的地址 --&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;fs.defaultFS&lt;/name&gt;<br>        &lt;value&gt;hdfs://hadoop102:8020&lt;/value&gt;<br>    &lt;/property&gt;<br><br>    &lt;!-- 指定hadoop数据的存储目录 --&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br>        &lt;value&gt;/opt/module/hadoop-3.1.3/data&lt;/value&gt;<br>    &lt;/property&gt;<br><br>    &lt;!-- 配置HDFS网页登录使用的静态用户为atguigu --&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;<br>        &lt;value&gt;atguigu&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br><br></code></pre></td></tr></table></figure>
<p>（2）HDFS配置文件</p>
<p>配置hdfs-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim hdfs-site.xml<br>文件内容如下：<br>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;<br>&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;<br><br>&lt;configuration&gt;<br>	&lt;!-- nn web端访问地址--&gt;<br>	&lt;property&gt;<br>        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;<br>        &lt;value&gt;hadoop102:9870&lt;/value&gt;<br>    &lt;/property&gt;<br>	&lt;!-- 2nn web端访问地址--&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;<br>        &lt;value&gt;hadoop104:9868&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br><br></code></pre></td></tr></table></figure>
<p>（3）YARN配置文件</p>
<p>配置yarn-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim yarn-site.xml<br>文件内容如下：<br>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;<br>&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;<br><br>&lt;configuration&gt;<br>    &lt;!-- 指定MR走shuffle --&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br>        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br>    &lt;/property&gt;<br><br>    &lt;!-- 指定ResourceManager的地址--&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br>        &lt;value&gt;hadoop103&lt;/value&gt;<br>    &lt;/property&gt;<br><br>    &lt;!-- 环境变量的继承 --&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;<br>        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br><br></code></pre></td></tr></table></figure>
<p>（4）MapReduce配置文件</p>
<p>配置mapred-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim mapred-site.xml<br>文件内容如下：<br>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;<br>&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;<br><br>&lt;configuration&gt;<br>	&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;<br>    &lt;property&gt;<br>        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br>        &lt;value&gt;yarn&lt;/value&gt;<br>    &lt;/property&gt;<br>&lt;/configuration&gt;<br><br></code></pre></td></tr></table></figure>
<p><strong>4）在集群上分发配置好的Hadoop配置文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">xsync /opt/module/hadoop-3.1.3/etc/hadoop/<br></code></pre></td></tr></table></figure>
<p><strong>5）去103和104上查看文件分发情况</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[atguigu@hadoop103 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml<br>[atguigu@hadoop104 ~]$ cat /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml<br></code></pre></td></tr></table></figure>
<h3><a name="4-2-4-" class="anchor" href="#4-2-4-"><span class="header-link"></span></a>4.2.4 建起集群</h3><p><strong>1）配置workers</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">[atguigu@hadoop102 hadoop]$ vim /opt/module/hadoop-3.1.3/etc/hadoop/workers<br>在该文件中增加如下内容：<br>hadoop102<br>hadoop103<br>hadoop104<br>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。<br><br>同步所有节点配置文件<br>[atguigu@hadoop102 hadoop]$ xsync /opt/module/hadoop-3.1.3/etc<br></code></pre></td></tr></table></figure>
<p><strong>2）启动集群</strong></p>
<p>（1）<strong>如果集群是第一次启动</strong>，需要在hadoop102节点格式化NameNode（<strong><em>注意：格式化NameNode，会产生新的集群id，导致NameNode和DataNode的集群id不一致，集群找不到已往数据。如果集群在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化。</em></strong>）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[atguigu@hadoop102 hadoop-3.1.3]$ hdfs namenode -format<br></code></pre></td></tr></table></figure>
<p>（2）启动HDFS</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[atguigu@hadoop102 hadoop-3.1.3]$ sbin/start-dfs.sh<br></code></pre></td></tr></table></figure>
<p>（3）在配置了<strong>ResourceManager</strong>的节点（hadoop103）启动YARN</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[atguigu@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh<br></code></pre></td></tr></table></figure>
<p> (4）Web端查看HDFS的NameNode</p>
<p>（a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop102:9870">http://hadoop102:9870</a></p>
<p>（b）查看HDFS上存储的数据信息</p>
<p>  (5）Web端查看YARN的ResourceManager</p>
<p>（a）浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop103:8088">http://hadoop103:8088</a></p>
<p>（b）查看YARN上运行的Job信息</p>
<p><strong>3）集群基本测试</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs shell">（1）上传文件到集群<br>上传小文件<br>[atguigu@hadoop102 ~]$ hadoop fs -mkdir /input<br>[atguigu@hadoop102 ~]$ hadoop fs -put $HADOOP_HOME/wcinput/word.txt /input<br><br>上传大文件<br>[atguigu@hadoop102 ~]$ hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /<br><br>（2）上传文件后查看文件存放在什么位置<br>查看HDFS文件存储路径<br>[atguigu@hadoop102 subdir0]$ pwd<br>/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1436128598-192.168.10.102-1610603650062/current/finalized/subdir0/subdir0<br>查看HDFS在磁盘存储文件内容<br>[atguigu@hadoop102 subdir0]$ cat blk_1073741825<br>hadoop yarn<br>hadoop mapreduce <br>atguigu<br>atguigu<br><br>（3）拼接<br>-rw-rw-r--. 1 atguigu atguigu 134217728 5月  23 16:01 blk_1073741836<br>-rw-rw-r--. 1 atguigu atguigu   1048583 5月  23 16:01 blk_1073741836_1012.meta<br>-rw-rw-r--. 1 atguigu atguigu  63439959 5月  23 16:01 blk_1073741837<br>-rw-rw-r--. 1 atguigu atguigu    495635 5月  23 16:01 blk_1073741837_1013.meta<br><br>[atguigu@hadoop102 subdir0]$ cat blk_1073741836&gt;&gt;tmp.tar.gz<br>[atguigu@hadoop102 subdir0]$ cat blk_1073741837&gt;&gt;tmp.tar.gz<br>[atguigu@hadoop102 subdir0]$ tar -zxvf tmp.tar.gz<br><br>（4）下载<br>[atguigu@hadoop104 software]$ hadoop fs -get /jdk-8u212-linux-x64.tar.gz ./<br><br>（5）执行wordcount程序<br>[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output<br></code></pre></td></tr></table></figure>
<h3><a name="4-2-5-" class="anchor" href="#4-2-5-"><span class="header-link"></span></a>4.2.5 配置历史服务器</h3><p>为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs shell">1）配置mapred-site.xml<br>[atguigu@hadoop102 hadoop]$ vim mapred-site.xml<br>在该文件里面增加如下配置。<br>&lt;!-- 历史服务器端地址 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;<br>    &lt;value&gt;hadoop102:10020&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 历史服务器web端地址 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;<br>    &lt;value&gt;hadoop102:19888&lt;/value&gt;<br>&lt;/property&gt;<br><br>2）分发配置<br>[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/mapred-site.xml<br><br>3）在hadoop102启动历史服务器<br>[atguigu@hadoop102 hadoop]$ mapred --daemon start historyserver<br><br>4）查看历史服务器是否启动<br>[atguigu@hadoop102 hadoop]$ jps<br><br>5）查看JobHistory<br>http://hadoop102:19888/jobhistory<br></code></pre></td></tr></table></figure>
<h3><a name="4-2-6-" class="anchor" href="#4-2-6-"><span class="header-link"></span></a>4.2.6 配置日志的聚集</h3><p>日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上。</p>
<p>日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试。</p>
<p><strong><em>注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryServer。</em></strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs xml">开启日志聚集功能具体步骤如下：<br>1）配置yarn-site.xml<br>[atguigu@hadoop102 hadoop]$ vim yarn-site.xml<br>在该文件里面增加如下配置。<br><span class="hljs-comment">&lt;!-- 开启日志聚集功能 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-comment">&lt;!-- 设置日志聚集服务器地址 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log.server.url<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>http://hadoop102:19888/jobhistory/logs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-comment">&lt;!-- 设置日志保留时间为7天 --&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>604800<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>2）分发配置<br>[atguigu@hadoop102 hadoop]$ xsync $HADOOP_HOME/etc/hadoop/yarn-site.xml<br><br>3）关闭NodeManager 、ResourceManager和HistoryServer<br>[atguigu@hadoop103 hadoop-3.1.3]$ sbin/stop-yarn.sh<br>[atguigu@hadoop103 hadoop-3.1.3]$ mapred --daemon stop <br>historyserver<br><br>4）启动NodeManager 、ResourceManage和HistoryServer<br>[atguigu@hadoop103 ~]$ start-yarn.sh<br>[atguigu@hadoop102 ~]$ mapred --daemon start historyserver<br><br>5）删除HDFS上已经存在的输出文件<br>[atguigu@hadoop102 ~]$ hadoop fs -rm -r /output<br><br>6）执行WordCount程序<br>[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output<br><br>7）查看日志<br>	（1）历史服务器地址<br>http://hadoop102:19888/jobhistory<br></code></pre></td></tr></table></figure>
<h3><a name="4-2-7-" class="anchor" href="#4-2-7-"><span class="header-link"></span></a>4.2.7集群启动/停止方式总结</h3><p><strong>1）各个模块分开启动/停止（配置ssh是前提）常用</strong></p>
<p>​    （1）整体启动/停止HDFS</p>
<p>start-dfs.sh/stop-dfs.sh</p>
<p>​    （2）整体启动/停止YARN</p>
<p>start-yarn.sh/stop-yarn.sh</p>
<p><strong>2）各个服务组件逐一启动/停止</strong></p>
<p>​    （1）分别启动/停止HDFS组件</p>
<p>hdfs --daemon start/stop namenode/datanode/secondarynamenode</p>
<p>​    （2）启动/停止YARN</p>
<p>yarn --daemon start/stop resourcemanager/nodemanager</p>
<h4><a name="4-2-7-1-hadoop-" class="anchor" href="#4-2-7-1-hadoop-"><span class="header-link"></span></a>4.2.7.1编写Hadoop集群常用脚本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs shell">1）Hadoop集群启停脚本（包含HDFS，Yarn，Historyserver）：myhadoop.sh<br>[atguigu@hadoop102 ~]$ cd /home/atguigu/bin<br>[atguigu@hadoop102 bin]$ vim myhadoop.sh<br>	输入如下内容<br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><br>if [ $# -lt 1 ]<br>then<br>    echo &quot;No Args Input...&quot;<br>    exit ;<br>fi<br><br>case $1 in<br>&quot;start&quot;)<br>        echo &quot; =================== 启动 hadoop集群 ===================&quot;<br><br>        echo &quot; --------------- 启动 hdfs ---------------&quot;<br>        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/start-dfs.sh&quot;<br>        echo &quot; --------------- 启动 yarn ---------------&quot;<br>        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/start-yarn.sh&quot;<br>        echo &quot; --------------- 启动 historyserver ---------------&quot;<br>        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver&quot;<br>;;<br>&quot;stop&quot;)<br>        echo &quot; =================== 关闭 hadoop集群 ===================&quot;<br><br>        echo &quot; --------------- 关闭 historyserver ---------------&quot;<br>        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver&quot;<br>        echo &quot; --------------- 关闭 yarn ---------------&quot;<br>        ssh hadoop103 &quot;/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh&quot;<br>        echo &quot; --------------- 关闭 hdfs ---------------&quot;<br>        ssh hadoop102 &quot;/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh&quot;<br>;;<br>*)<br>    echo &quot;Input Args Error...&quot;<br>;;<br>esac<br>	保存后退出，然后赋予脚本执行权限<br>[atguigu@hadoop102 bin]$ chmod +x myhadoop.sh<br><br>2）查看三台服务器Java进程脚本：jpsall<br>[atguigu@hadoop102 ~]$ cd /home/atguigu/bin<br>[atguigu@hadoop102 bin]$ vim jpsall<br>	输入如下内容<br><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><br>for host in hadoop102 hadoop103 hadoop104<br>do<br>        echo =============== $host ===============<br>        ssh $host jps <br>done<br>	保存后退出，然后赋予脚本执行权限<br>[atguigu@hadoop102 bin]$ chmod +x jpsall<br><br>3）分发/home/atguigu/bin目录，保证自定义脚本在三台机器上都可以使用<br>[atguigu@hadoop102 ~]$ xsync /home/atguigu/bin/<br><br></code></pre></td></tr></table></figure>
<h3><a name="4-2-8-" class="anchor" href="#4-2-8-"><span class="header-link"></span></a>4.2.8 常用端口号说明</h3><table>
<thead>
<tr>
<th>端口名称</th>
<th>Hadoop2.x</th>
<th>Hadoop3.x</th>
</tr>
</thead>
<tbody>
<tr>
<td>NameNode内部通信端口</td>
<td>8020 / 9000</td>
<td>8020 /  9000/9820</td>
</tr>
<tr>
<td>NameNode HTTP UI</td>
<td>50070</td>
<td>9870</td>
</tr>
<tr>
<td>MapReduce查看执行任务端口</td>
<td>8088</td>
<td>8088</td>
</tr>
<tr>
<td>历史服务器通信端口</td>
<td>19888</td>
<td>19888</td>
</tr>
</tbody>
</table>
<h3><a name="4-2-9-" class="anchor" href="#4-2-9-"><span class="header-link"></span></a>4.2.9 集群时间同步</h3><p>如果服务器在公网环境（能连接外网），可以不采用集群时间同步，因为服务器会定期和公网时间进行校准；</p>
<p>如果服务器在内网环境，必须要配置集群时间同步，否则时间久了，会产生时间偏差，导致集群执行任务时间不同步。</p>
<p><strong>1）需求</strong></p>
<p>找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，生产环境根据任务对时间的准确程度要求周期同步。测试环境为了尽快看到效果，采用1分钟同步一次。</p>
<p><strong>2）时间服务器配置（必须root用户）</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs shell">（1）查看所有节点ntpd服务状态和开机自启动状态<br>[atguigu@hadoop102 ~]$ sudo systemctl status ntpd<br>[atguigu@hadoop102 ~]$ sudo systemctl start ntpd<br>[atguigu@hadoop102 ~]$ sudo systemctl is-enabled ntpd<br><br><br>（2）修改hadoop102的ntp.conf配置文件<br>[atguigu@hadoop102 ~]$ sudo vim /etc/ntp.conf<br>修改内容如下<br>（a）修改1（授权192.168.10.0-192.168.10.255网段上的所有机器可以从这台机器上查询和同步时间）<br><span class="hljs-meta prompt_">#</span><span class="language-bash">restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap</span><br>为restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap<br>	（b）修改2（集群在局域网中，不使用其他互联网上的时间）<br>server 0.centos.pool.ntp.org iburst<br>server 1.centos.pool.ntp.org iburst<br>server 2.centos.pool.ntp.org iburst<br>server 3.centos.pool.ntp.org iburst<br>为<br><span class="hljs-meta prompt_">#</span><span class="language-bash">server 0.centos.pool.ntp.org iburst</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">server 1.centos.pool.ntp.org iburst</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">server 2.centos.pool.ntp.org iburst</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">server 3.centos.pool.ntp.org iburst</span><br>（c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步）<br>server 127.127.1.0<br>fudge 127.127.1.0 stratum 10<br><br><br>（3）修改hadoop102的/etc/sysconfig/ntpd 文件<br>[atguigu@hadoop102 ~]$ sudo vim /etc/sysconfig/ntpd<br>增加内容如下（让硬件时间与系统时间一起同步）<br>SYNC_HWCLOCK=yes<br><br><br>（4）重新启动ntpd服务<br>[atguigu@hadoop102 ~]$ sudo systemctl start ntpd<br><br><br>（5）设置ntpd服务开机启动<br>[atguigu@hadoop102 ~]$ sudo systemctl enable ntpd<br></code></pre></td></tr></table></figure>
<p><strong>3）其他机器配置（必须root用户）</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell">（1）关闭所有节点上ntp服务和自启动<br>[atguigu@hadoop103 ~]$ sudo systemctl stop ntpd<br>[atguigu@hadoop103 ~]$ sudo systemctl disable ntpd<br>[atguigu@hadoop104 ~]$ sudo systemctl stop ntpd<br>[atguigu@hadoop104 ~]$ sudo systemctl disable ntpd<br><br>（2）在其他机器配置1分钟与时间服务器同步一次<br>[atguigu@hadoop103 ~]$ sudo crontab -e<br>编写定时任务如下：<br>*/1 * * * * /usr/sbin/ntpdate hadoop102<br><br>（3）修改任意机器时间<br>[atguigu@hadoop103 ~]$ sudo date -s &quot;2021-9-11 11:11:11&quot;<br>（4）1分钟后查看机器是否与时间服务器同步<br>[atguigu@hadoop103 ~]$ sudo date<br></code></pre></td></tr></table></figure>
<h2><a name="5-" class="anchor" href="#5-"><span class="header-link"></span></a>5 常见错误及解决方案</h2><p>1）防火墙没关闭、或者没有启动YARN</p>
<p><em>INFO client.RMProxy: Connecting to ResourceManager at hadoop108/192.168.10.108:8032</em></p>
<p>2）主机名称配置错误</p>
<p>3）IP地址配置错误</p>
<p>4）ssh没有配置好</p>
<p>5）root用户和atguigu两个用户启动集群不统一</p>
<p>6）配置文件修改不细心</p>
<p>7）不识别主机名称</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java">java.net.UnknownHostException: hadoop102: hadoop102<br>        at java.net.InetAddress.getLocalHost(InetAddress.java:<span class="hljs-number">1475</span>)<br>        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:<span class="hljs-number">146</span>)<br>        at org.apache.hadoop.mapreduce.Job$<span class="hljs-number">10.</span>run(Job.java:<span class="hljs-number">1290</span>)<br>        at org.apache.hadoop.mapreduce.Job$<span class="hljs-number">10.</span>run(Job.java:<span class="hljs-number">1287</span>)<br>        at java.security.AccessController.doPrivileged(Native Method)<br>at javax.security.auth.Subject.doAs(Subject.java:<span class="hljs-number">415</span>)<br><br></code></pre></td></tr></table></figure>
<p>解决办法：</p>
<p>（1）在/etc/hosts文件中添加192.168.10.102 hadoop102</p>
<p>​    （2）主机名称不要起hadoop hadoop000等特殊名称</p>
<p>8）DataNode和NameNode进程同时只能工作一个。</p>
<p>9）执行命令不生效，粘贴Word中命令时，遇到-和长–没区分开。导致命令失效</p>
<p>解决办法：尽量不要粘贴Word中代码。</p>
<p>10）jps发现进程已经没有，但是重新启动集群，提示进程已经开启。</p>
<p>原因是在Linux的根目录下/tmp目录中存在启动的进程临时文件，将集群相关进程删除掉，再重新启动集群。</p>
<p>11）jps不生效</p>
<p>原因：全局变量hadoop java没有生效。解决办法：需要source /etc/profile文件。</p>
<p>12）8088端口连接不上</p>
<p>[atguigu@hadoop102 桌面]$ cat /etc/hosts</p>
<p>注释掉如下代码</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">::1         hadoop102</span><br></code></pre></td></tr></table></figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="category-chain-item">大数据</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="print-no-link">#大数据</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>hadoop(入门)</div>
      <div>http://example.com/2024/03/12/hadoop-入门/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>HRX</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年3月12日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/03/20/Liunx1/" title="Liunx1">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Liunx1</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/12/23/%E6%BC%AB%E8%B0%881/" title="漫谈">
                        <span class="hidden-mobile">漫谈</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="waline"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#waline', function() {
      Fluid.utils.createCssLink('https://cdn.staticfile.org/waline/2.15.5/waline.min.css')
      Fluid.utils.createScript('https://cdn.staticfile.org/waline/2.15.5/waline.min.js', function() {
        var options = Object.assign(
          {"serverURL":"https://vehzgbm2.api.lncldglobal.com","path":"window.location.pathname","meta":["nick","mail","link"],"requiredMeta":["nick"],"lang":"zh-CN","emoji":["https://cdn.jsdelivr.net/gh/walinejs/emojis/weibo"],"dark":"html[data-user-color-scheme=\"dark\"]","wordLimit":0,"pageSize":10},
          {
            el: '#waline',
            path: window.location.pathname
          }
        )
        Waline.init(options);
        Fluid.utils.waitElementVisible('#waline .vcontent', () => {
          var imgSelector = '#waline .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
